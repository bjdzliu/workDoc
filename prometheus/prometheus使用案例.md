# prometheus使用案例之Canonical

Canonical是一家软件公司,Ubuntu就是他们最出名的产品.同时他们也使用OpenStack为客户提供私有云的搭建服务

![](http://images.51cto.com/files/uploadimg/20110406/0943460.png)


在使用prometheus之前,他们公司使用的监控系统基于Nagios,Graphite/Statsd,以及一个内部的Django应用搭建的,但是这一套系统并无法为Canonical内部以及他们客户的云端环境提供的足够的灵活性以及反馈.


他们评估的几种选择,包括InfluxDB以及对Graphite进行扩展等方案,但在第一次尝试prometheus后,他们发现prometheus就是Canonical一直在寻找的简单和力量的组合.而且,他们公司的几位在谷歌工作的员工有Borgmon(prometheus就是模范Borgmon的开源实现)的使用经验,这大大激发他们的兴趣


选择一个exporter花费了他们一段时间.最开始时他们想用collectd但是发现这样会导致一些限制.现在他们自己在开发openstack-exporter,但是发现还没有一个足够好,能够工作的例子帮助我们编写来自scratch的exporter



在使用大量exporter后,他们发现编写一个能够提供足够有用指标的exporter非常容易,比如他们开发一个使用于云端环境的openstack-exporter,而且很快各个开发团队都开始接受prometheus了


-----



# prometheus使用案例之DigitalOcean

DigitalOcean在PromCon 2016做了一次演讲,介绍了他们公司内部是如何使用prometheus做监控系统的.


![](https://prometheus.io/assets/blog/2016-09-14/DO_Logo_Horizontal_Blue-3db19536-cb89e8e1298.png)

DigitalOcean是美国一家公有云公司,目前业务体量上分布在13个区域中的2千万个SSD cloud servers.


在使用prometheus之前,他们使用Graphite和OpenTSDB,Graphite用在小规模的应用上,而OpenTSDB则通过collectd做数据采集器,收集所有物理server的指标.同时也使用Nagios做警报系统.目前他们还在使用Graphite,但已经不适用OpenTSDB了.


对于OpenTSDB,主要的原因很难让它在集群内有大量数据流入的时候保持在线状态,这个时候我们团队不得不启用新的服务提高集群的容量.后来我们使用了类似黑名单/白名单的功能,但还是无法得到真正的改善.同时其他团队开始抱怨查询语言和可视化工具是多么得令人失望.


之前是通过Collectd采集数据后发送到OpenTSDB,然后开始试验性得尝试让prometheus与collectd并行运行,同时也创建几个自定义的exporter用来采集SSD Cloud Server的一些指标.再后来prometheus的系统能为我们提供与OpenTSDB相同的特性的时候,开始关闭Collectd以及OpenTSDB集群.

prometheus开始DigitalOcean正式使用后,他们开发一些工具,让prometheus server和一些通用的exporter能够更加容易使用

最大的提升在于他们对hypervisor machines有了更多的了解.collectd和node exporter输出的数据的一样的,但是对于Go开发团队来说,他们能够更容易得定制每一台hyperviso的输出.同时也意味着接下来在学习和指导别人创建满足需求的指标时会更加容易.


还是致力于让prometheus更加易用和自动化这方面,让其他团队的成员能够只关心查询和警告这两方面,比如在服务和数据库方面.

DigitalOcean也开源一个工具[vulcan](https://github.com/digitalocean/vulcan)用于保存长期数据,同时保留了PromQL


# prometheus使用案例之ShuttleCloud